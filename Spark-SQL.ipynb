{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931df07b-115f-495a-a9c4-7553d6560923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6edd07-923b-43ee-a193-fa7499140bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301cf593-5b54-4339-8f7f-ab5dc60998b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b80beb-f474-486b-ad75-db7bb6933a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every application, there is only one Instance of SparlSession/SparkContext\n",
    "spark = SparkSession.builder\\\n",
    "                   .appName(\"DF Operations\")\\\n",
    "                   .master(\"local[2]\")\\\n",
    "                   .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e37f6-f577-48d4-b723-fcff2e072f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2fe10-d4f2-4e5c-af03-934e24c3e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.read = returns DataFrameReader\n",
    "\n",
    "df = spark.read\\\n",
    "          .option(\"header\",True)\\\n",
    "          .option(\"inferSchema\",True)\\\n",
    "          .csv(\"d:\\\\data\\\\custs.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898abd6d-22b1-4652-baeb-5e21c2590bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5524346-ae3a-4421-b26a-4f81808b828d",
   "metadata": {},
   "source": [
    "## String / SQL Like Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2ad21-0ec5-46e3-ab4a-41d1968681ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get all the records where age > 40\n",
    "# 2. get all the records where fname starts with 'S'\n",
    "# 3. get all the records where desig is ('Teacher',\"Pilot','Lawyer')\n",
    "# 4. get all the records where age > 50 and desig is Pilot\n",
    "# 5. get all the records where age is between 40 and 50\n",
    "# 6. Get all the records where desig is null \n",
    "                                       \n",
    "# 7. Get designation wise count\n",
    "# 8. Get top 10 designations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b68129-1cb5-4132-bc6a-a8554b387382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get all the records where age > 40\n",
    "df.where(\"age > 40\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f48065-29df-48c9-9a51-6309b9e89542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. get all the records where fname starts with 'S'\n",
    "df.where(\"fname like 'S%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cef3a9-2c19-4e42-8ad0-7c8573a5d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. get all the records where desig is ('Teacher',\"Pilot','Lawyer')\n",
    "df.where(\"desig in ('Teacher','Pilot','Lawyer')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0795e51-2ee4-4d59-b5b9-0f2b3c658848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. get all the records where age > 50 and desig is Pilot\n",
    "df.where(\"desig = 'Pilot' and age > 50\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3774012-134d-4206-85d6-2e163171f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. get all the records where age is between 40 and 50\n",
    "df.where(\"age between 40 and 50\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a70ae5-1a81-4d5b-8eb1-dd7411673d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Get designation wise count\n",
    "# select desig,count(*) from df group by desig ;\n",
    "\n",
    "df.groupby(\"desig\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bba1fd-5701-4170-978e-4221af55c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Get top 10 designations\n",
    "df.groupby(\"desig\").count().orderBy(\"count\",ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de1bdf9-e014-4f2a-8c6f-84a7840eb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Get top 10 designations where overall count is greater than 215\n",
    "df.groupby(\"desig\").count().orderBy(\"count\",ascending=False).where(\"count > 215\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5bf78b-7177-41fc-938a-6bef9ae3ffc2",
   "metadata": {},
   "source": [
    "## COLUMN Based Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd9143-87f0-4e7f-994a-d9d4f69616b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ba1d7-3807-425a-a742-ea564f493ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.where(col(\"age\") > 40).show()\n",
    "# df.select(\"fname\",\"lname\",\"age\").where(\"age > 40\").show()\n",
    "\n",
    "# 2. get all the records where fname starts with 'S'\n",
    "# df.where(col(\"fname\").startswith(\"S\")).show()\n",
    "\n",
    "# 3. get all the records where desig is ('Teacher',\"Pilot','Lawyer')\n",
    "# df.where(col(\"desig\").isin('Teacher','Pilot','Lawyer')).show()\n",
    "\n",
    "# 4. get all the records where age > 50 and desig is Pilot\n",
    "# df.where((col(\"age\") > 50) & (col(\"desig\")=='Pilot')).show()\n",
    "\n",
    "# 5. get all the records where age is between 40 and 50\n",
    "# df.where(col(\"age\").between(40,42)).show()\n",
    "\n",
    "# 6. Get all the records where desig is null\n",
    "df.where(col(\"desig\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d977e0-43a7-4e08-850b-fbb6096b768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c6bc8-fe3c-4365-82d4-47224ff21871",
   "metadata": {},
   "source": [
    "### Disadvantage of Using InferSchema option :\n",
    "Spark runs a seperate JOB to figure out the schema (schema will be correct if date is not in default format), which can be avoided by creating your own Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84c1d8-a6a6-4821-b763-29920b96ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_df = spark.read\\\n",
    "          .option(\"header\",True)\\\n",
    "          .option(\"inferSchema\",True)\\\n",
    "          .csv(\"d:\\\\data\\\\txn_with_header.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa2f6ed-f4a4-4189-a45c-b937ef0bd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4e151-662c-4dcd-808c-55806791cdad",
   "metadata": {},
   "source": [
    "### How to Define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2239f9-3e04-42e1-805c-1b55fcc84d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,FloatType,DateType,TimestampType,ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d113133-924c-4381-bb34-1e9bcf82fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custSchema = StructType([\n",
    "                        StructField(\"cid\",IntegerType()),\n",
    "                        StructField(\"fname\",StringType()),\n",
    "                        StructField(\"lname\",StringType()),\n",
    "                        StructField(\"age\",IntegerType()),\n",
    "                        StructField(\"desig\",StringType()) \n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2892569-1146-4310-b88f-bcf6450461d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF = spark.read\\\n",
    "          .option(\"header\",True)\\\n",
    "          .schema(custSchema)\\\n",
    "          .csv(\"d:\\\\data\\\\custs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc642180-fff7-40ca-b67a-92cb1ccecf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d572732-a2fc-45a5-b627-d861d7da0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fail to parse '06-26-2011' in the new parser. \n",
    "# You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, \n",
    "# or set to \"CORRECTED\" and treat it as an invalid datetime string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c2a23-39f1-4284-8a71-11207e5fb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffbb2b-f015-4fde-b1be-a6b88c6f5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "txnSchema = StructType([\n",
    "                        StructField(\"txn_id\",IntegerType()),\n",
    "                        StructField(\"txn_date\",DateType()),\n",
    "                        StructField(\"cid\",StringType()),\n",
    "                        StructField(\"amount\",FloatType()),\n",
    "                        StructField(\"prod_cat\",StringType()), \n",
    "                        StructField(\"prod\",StringType()),\n",
    "                        StructField(\"city\",StringType()),\n",
    "                        StructField(\"state\",StringType()),\n",
    "                        StructField(\"mode\",StringType())\n",
    "                      ])\n",
    "\n",
    "# ,\n",
    "                        # StructField(\"bad_records\",StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a2f33-eb5a-42e8-8217-ced5a921495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bc056-be76-4cd6-aed1-22f2e697cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05-26-2011\n",
    "txnDF = spark.read\\\n",
    "          .option(\"header\",True)\\\n",
    "          .schema(txnSchema)\\\n",
    "          .option(\"mode\",\"PERMISSIVE\")\\\n",
    "          .option(\"dateFormat\",\"M-dd-y\")\\\n",
    "          .csv(\"d:\\\\data\\\\txn_with_header.txt\")\n",
    "\n",
    "txnDF.show(2,truncate=False)\n",
    "\n",
    "# .option(\"columnNameOfCorruptRecord\",\"bad_records\")\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c3d2e-f9e0-4fb8-8c77-34eda6e6392c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise :\n",
    "\n",
    "* Create DF reading file <B>date_format_3.txt</B> \n",
    "<br>name,doj\n",
    "<br>Ankit,18/03/2023 13:10\n",
    "* Show Schema (make sure 2nd field is of Date Type)\n",
    "* Print Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28ad37-1acc-424b-8a36-0a41ba01bff3",
   "metadata": {},
   "source": [
    "### Reading JSON file with simple and Complex Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46392efd-d24c-4cf4-8a20-b0fba5884353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18/03/2023 13:10\n",
    "dtSchema = StructType([\n",
    "                        StructField(\"name\",StringType()),\n",
    "                        StructField(\"doj\",TimestampType()) \n",
    "                      ])\n",
    "\n",
    "dateDF = spark.read\\\n",
    "          .option(\"header\",True)\\\n",
    "          .schema(dtSchema)\\\n",
    "          .option(\"timestampFormat\",\"d/M/y HH:mm\")\\\n",
    "          .csv(\"d:\\\\data\\\\dates\\\\date_format_3.txt\")\n",
    "\n",
    "dateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc11be1-391b-49db-a5f5-f0cebf79ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonSchema = StructType([\n",
    "                        StructField(\"name\",StringType()),\n",
    "                        StructField(\"age\",IntegerType()),\n",
    "                        StructField(\"hobbies\",ArrayType(StringType())),\n",
    "                        StructField(\"address\",StructType([\n",
    "                                                          StructField(\"rno\",IntegerType()),\n",
    "                                                          StructField(\"city\",StringType())\n",
    "                                                          ]))                       \n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82a0e6-220f-4a40-bfa9-81edd7b6590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.read\\\n",
    "          .option(\"header\",True)\\\n",
    "          .schema(jsonSchema)\\\n",
    "          .json(\"d:\\\\data\\\\people.json\")\n",
    "\n",
    "jsonDF.select(\"name\",\"address.city\").show()\n",
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1c21f-50e5-4eec-9be1-1366c176ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d20039-edc8-4a57-8abe-9888b22fe89d",
   "metadata": {},
   "source": [
    "### Read Mode : \n",
    "* a) PERMISSIVE \n",
    "* b)DROPMALFORMED and \n",
    "* c)FAILFAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039b6a8-73a9-45ab-a4bc-fbaf80211fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txnDF.show(2)\n",
    "# PERMISSIVE -> replace corrupt record with null values\n",
    "# DROPMALFORMED -> Drop records if it is not as per defined schema\n",
    "# FAILFAST -> Throw Exception when it come across corrupt records, it will not process further\n",
    "\n",
    "# txnDF = spark.read\\\n",
    "#           .option(\"header\",True)\\\n",
    "#           .schema(txnSchema)\\\n",
    "#           .option(\"mode\",\"FAILFAST\")\\\n",
    "#           .option(\"dateFormat\",\"M-dd-y\")\\\n",
    "#           .csv(\"d:\\\\data\\\\txn_with_header.txt\")\n",
    "\n",
    "# txnDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb6fed6-8076-4021-b517-598e0347535e",
   "metadata": {},
   "source": [
    "### Simple Aggregation(Table/DF Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966d1d0-9dc7-4a7e-b836-b93154e56169",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d21f9c-cbb7-44dd-9846-455a023a27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum,avg,max,min,round,count\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe2b3c-1fe7-4301-819e-02bd7f4b4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custDF.select(f.avg(\"age\"),f.min(\"age\"),f.max(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4ebeb5-5619-4cde-a4a5-a84c0854ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "txnDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030e684-20c3-4f1d-aff1-1a8e772fdfd0",
   "metadata": {},
   "source": [
    "### Multiple Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673cb947-bad8-4c52-a688-eb8a0d2cd93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State,City Wise Sale\n",
    "total_sale = f.round(f.sum(\"amount\"),2).alias(\"Total_Sale\")\n",
    "average_sale = f.round(f.avg(\"amount\"),2).alias(\"Avg_Sale\")\n",
    "\n",
    "txnDF.groupby(\"state\")\\\n",
    "        .agg(\n",
    "            total_sale,\n",
    "            average_sale,\n",
    "            f.min(\"amount\"),\n",
    "            f.max(\"amount\")\n",
    "        ).show()\n",
    "# average Sale\n",
    "# Min Sale\n",
    "# Max Sale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8e278-f6e2-42dd-8787-85b325f4957a",
   "metadata": {},
   "source": [
    "### Window Aggregration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191cda8-1c4f-48ef-85e5-0c034923d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_state_city_sum = txnDF.groupby(\"state\",\"city\").agg(sum(\"amount\").alias(\"totalSale\")).orderBy(\"state\",\"city\")\n",
    "txn_state_city_sum.show(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20670a0a-8284-48be-b286-66ff09dea506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651ad32-ac6a-40fa-b818-472bfa3fc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_total_window = Window.partitionBy(\"state\")\\\n",
    "                             .orderBy(\"city\")\\\n",
    "                             .rowsBetween(Window.unboundedPreceding,Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57253317-bcb6-4595-b074-97ef379ac4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_state_city_sum.withColumn(\"RunningTotal\",f.sum(\"totalSale\").over(running_total_window)).orderBy(\"state\",\"city\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a77564-dcf1-4bc5-892a-4e2c543e1e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myWindow = Window.partitionBy(\"state\")\\\n",
    "                             .orderBy(\"totalSale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b04a00-82a9-4a17-90ca-ff8940429de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_state_city_sum_top_3 = txn_state_city_sum.withColumn(\"Rank\",f.rank().over(myWindow)).orderBy(\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7b9ed-e178-4051-86d6-625a570b6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_state_city_sum_top_3.where(\"Rank < 4\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a1d9d-4d08-4389-bf37-c233ee39f172",
   "metadata": {},
   "source": [
    "### How to write DF onto Disk = DataFrameWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38864e1e-d266-4a28-b4f4-79a0d3e78df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txnDF.write\\\n",
    "     .mode(\"overwrite\")\\\n",
    "     .format(\"json\")\\\n",
    "     .save(\"d:\\\\data\\\\json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed35bc8a-b92d-4433-85ce-7b57cc0a1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "txnDF.write\\\n",
    "     .mode(\"overwrite\")\\\n",
    "     .format(\"parquet\")\\\n",
    "     .save(\"d:\\\\data\\\\parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4617327b-5d91-4646-abc6-32bf58ca24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "txn_par_df = spark.read.load(\"D:\\data\\parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a216a4c4-7bf0-4bf1-a8ce-bd6f29b4e490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|         state|\n",
      "+--------------+\n",
      "|    California|\n",
      "|    California|\n",
      "|     Wisconsin|\n",
      "|     Tennessee|\n",
      "|      Illinois|\n",
      "|South Carolina|\n",
      "|          Ohio|\n",
      "|          Iowa|\n",
      "|       Florida|\n",
      "|        Nevada|\n",
      "|          Ohio|\n",
      "|    California|\n",
      "|        Hawaii|\n",
      "|    California|\n",
      "|        Hawaii|\n",
      "|South Carolina|\n",
      "|      Nebraska|\n",
      "|          Utah|\n",
      "|    New Jersey|\n",
      "|     Louisiana|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txn_par_df.select(\"state\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17e5a056-36dc-4a96-bbfe-78c6d880b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+------+------------------+--------------------+----------+----------+------+\n",
      "|txn_id|  txn_date|    cid|amount|          prod_cat|                prod|      city|     state|  mode|\n",
      "+------+----------+-------+------+------------------+--------------------+----------+----------+------+\n",
      "|     1|2011-05-26|4006742| 98.44|Exercise & Fitness|Weightlifting Gloves|Long Beach|California|credit|\n",
      "|     2|2011-06-01|4009775|  5.58|Exercise & Fitness|Weightlifting Mac...|   Anaheim|California|credit|\n",
      "|     3|2011-06-05|4002199|198.19|        Gymnastics|    Gymnastics Rings| Milwaukee| Wisconsin|credit|\n",
      "+------+----------+-------+------+------------------+--------------------+----------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txn_par_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff827bc-e0d2-441c-a3fb-d170d3632ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     append: Append contents of this DataFrame to existing data.\n",
    "#     overwrite: Overwrite existing data.\n",
    "#     error or errorifexists: Throw an exception if data already exists.\n",
    "#     ignore: Silently ignore this operation if data already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "834784c4-0ba6-4ac0-b085-bf0d8cb688ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "txnDF.select(\"txn_id\",\"cid\",\"amount\")\\\n",
    "     .write\\\n",
    "     .mode(\"append\")\\\n",
    "     .format(\"parquet\")\\\n",
    "     .save(\"d:\\\\data\\\\parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
