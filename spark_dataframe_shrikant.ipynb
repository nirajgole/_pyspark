{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark as fs\n",
    "fs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Schemas\").master(\"local[2]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eid: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- yoj: integer (nullable = true)\n",
      " |-- dept_id: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# emp = eid,name,yoj,dept,gender,salary\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,DateType,ShortType,CharType\n",
    "empData=[\n",
    "    # (1,'James'),\n",
    "    (1,'James',2019,10,'M',45),\n",
    "    (2,'John',2020,40,'M',78),\n",
    "    (3,'Jessica',2021,50,'M',23),\n",
    "    (4,'Javier',2019,60,'M',12),\n",
    "    (5,'June',2023,10,'M',85)\n",
    "    ]\n",
    "\n",
    "empSchema = StructType(\n",
    "    [\n",
    "        StructField('eid',IntegerType()),\n",
    "        StructField('name',StringType()),\n",
    "        StructField('yoj',IntegerType()),\n",
    "        StructField('dept_id',IntegerType()),\n",
    "        StructField('gender',StringType()),\n",
    "        StructField('age',IntegerType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "empDf=spark.createDataFrame(empData,empSchema)\n",
    "empDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_id: integer (nullable = true)\n",
      " |-- dept: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptData=[\n",
    "    (10,'Sales'),\n",
    "    (20,'Marketing'),\n",
    "    (40,'IT'),\n",
    "    (30,'HR'),\n",
    "    ]\n",
    "\n",
    "deptSchema = StructType(\n",
    "    [\n",
    "        StructField('dept_id',IntegerType()),\n",
    "        StructField('dept',StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "deptDf=spark.createDataFrame(deptData,deptSchema)\n",
    "deptDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-------+------+---+-------+-----+\n",
      "|eid| name| yoj|dept_id|gender|age|dept_id| dept|\n",
      "+---+-----+----+-------+------+---+-------+-----+\n",
      "|  1|James|2019|     10|     M| 45|     10|Sales|\n",
      "|  5| June|2023|     10|     M| 85|     10|Sales|\n",
      "|  2| John|2020|     40|     M| 78|     40|   IT|\n",
      "+---+-----+----+-------+------+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "innerJoinDF = empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,'inner')\n",
    "innerJoinDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-------+------+---+-------+-----+\n",
      "|eid|   name| yoj|dept_id|gender|age|dept_id| dept|\n",
      "+---+-------+----+-------+------+---+-------+-----+\n",
      "|  2|   John|2020|     40|     M| 78|     40|   IT|\n",
      "|  1|  James|2019|     10|     M| 45|     10|Sales|\n",
      "|  5|   June|2023|     10|     M| 85|     10|Sales|\n",
      "|  3|Jessica|2021|     50|     M| 23|   null| null|\n",
      "|  4| Javier|2019|     60|     M| 12|   null| null|\n",
      "+---+-------+----+-------+------+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftJoinDF=empDf.join(deptDf,empDf.dept_id==deptDf.dept_id,'left')\n",
    "leftJoinDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+----+------+----+---------+\n",
      "|dept_id| eid|   name| yoj|gender| age|     dept|\n",
      "+-------+----+-------+----+------+----+---------+\n",
      "|     10|   1|  James|2019|     M|  45|    Sales|\n",
      "|     10|   5|   June|2023|     M|  85|    Sales|\n",
      "|     20|null|   null|null|  null|null|Marketing|\n",
      "|     30|null|   null|null|  null|null|       HR|\n",
      "|     40|   2|   John|2020|     M|  78|       IT|\n",
      "|     50|   3|Jessica|2021|     M|  23|     null|\n",
      "|     60|   4| Javier|2019|     M|  12|     null|\n",
      "+-------+----+-------+----+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullOuterJoinDF=empDf.join(deptDf,'dept_id','fullouter') \n",
    "#same as `USING`` keyword in mysql, no need to write `t1.c1==t2.c1` only column name if it is same in both tables\n",
    "fullOuterJoinDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-----+----+------+---+\n",
      "|dept_id|eid| name| yoj|gender|age|\n",
      "+-------+---+-----+----+------+---+\n",
      "|     10|  1|James|2019|     M| 45|\n",
      "|     10|  5| June|2023|     M| 85|\n",
      "|     40|  2| John|2020|     M| 78|\n",
      "+-------+---+-----+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftSemiJoin=empDf.join(deptDf,'dept_id','leftsemi')\n",
    "leftSemiJoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+-------+----+------+---+\n",
      "|dept_id|eid|   name| yoj|gender|age|\n",
      "+-------+---+-------+----+------+---+\n",
      "|     50|  3|Jessica|2021|     M| 23|\n",
      "|     60|  4| Javier|2019|     M| 12|\n",
      "+-------+---+-------+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftAntiJoin=empDf.join(deptDf,'dept_id','left_anti')\n",
    "leftAntiJoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function join in module pyspark.sql.dataframe:\n",
      "\n",
      "join(self, other: 'DataFrame', on: Union[str, List[str], pyspark.sql.column.Column, List[pyspark.sql.column.Column], NoneType] = None, how: Optional[str] = None) -> 'DataFrame'\n",
      "    Joins with another :class:`DataFrame`, using the given join expression.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : :class:`DataFrame`\n",
      "        Right side of the join\n",
      "    on : str, list or :class:`Column`, optional\n",
      "        a string for the join column name, a list of column names,\n",
      "        a join expression (Column), or a list of Columns.\n",
      "        If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      "        the column(s) must exist on both sides, and this performs an equi-join.\n",
      "    how : str, optional\n",
      "        default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      "        ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\n",
      "        ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\n",
      "        ``anti``, ``leftanti`` and ``left_anti``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        Joined DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The following performs a full outer join between ``df1`` and ``df2``.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> from pyspark.sql.functions import desc\n",
      "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")]).toDF(\"age\", \"name\")\n",
      "    >>> df2 = spark.createDataFrame([Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\n",
      "    >>> df3 = spark.createDataFrame([Row(age=2, name=\"Alice\"), Row(age=5, name=\"Bob\")])\n",
      "    >>> df4 = spark.createDataFrame([\n",
      "    ...     Row(age=10, height=80, name=\"Alice\"),\n",
      "    ...     Row(age=5, height=None, name=\"Bob\"),\n",
      "    ...     Row(age=None, height=None, name=\"Tom\"),\n",
      "    ...     Row(age=None, height=None, name=None),\n",
      "    ... ])\n",
      "    \n",
      "    Inner join on columns (default)\n",
      "    \n",
      "    >>> df.join(df2, 'name').select(df.name, df2.height).show()\n",
      "    +----+------+\n",
      "    |name|height|\n",
      "    +----+------+\n",
      "    | Bob|    85|\n",
      "    +----+------+\n",
      "    >>> df.join(df4, ['name', 'age']).select(df.name, df.age).show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    | Bob|  5|\n",
      "    +----+---+\n",
      "    \n",
      "    Outer join for both DataFrames on the 'name' column.\n",
      "    \n",
      "    >>> df.join(df2, df.name == df2.name, 'outer').select(\n",
      "    ...     df.name, df2.height).sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Bob|    85|\n",
      "    |Alice|  null|\n",
      "    | null|    80|\n",
      "    +-----+------+\n",
      "    >>> df.join(df2, 'name', 'outer').select('name', 'height').sort(desc(\"name\")).show()\n",
      "    +-----+------+\n",
      "    | name|height|\n",
      "    +-----+------+\n",
      "    |  Tom|    80|\n",
      "    |  Bob|    85|\n",
      "    |Alice|  null|\n",
      "    +-----+------+\n",
      "    \n",
      "    Outer join for both DataFrams with multiple columns.\n",
      "    \n",
      "    >>> df.join(\n",
      "    ...     df3,\n",
      "    ...     [df.name == df3.name, df.age == df3.age],\n",
      "    ...     'outer'\n",
      "    ... ).select(df.name, df3.age).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  2|\n",
      "    |  Bob|  5|\n",
      "    +-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check documentation for extra\n",
    "import pyspark\n",
    "help(pyspark.sql.DataFrame.join)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
